{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f596f236",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-14 15:25:32,789] A new study created in memory with name: no-name-3edc30d0-2827-4cf6-8f79-3dc919dc4a13\n",
      "/tmp/ipykernel_34091/3526829743.py:147: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  optimizer = optim.Adam(model.parameters(), lr=trial.suggest_loguniform('lr', 1e-5, 1e-3), weight_decay=trial.suggest_loguniform('weight_decay', 1e-6, 1e-4))\n",
      "[I 2024-08-14 15:25:54,334] Trial 0 finished with value: 57.0945945945946 and parameters: {'conv1_out_channels': 16, 'conv2_out_channels': 63, 'conv3_out_channels': 104, 'dropout': 0.39639874736170944, 'fc1_units': 302, 'lr': 0.0003804158783009216, 'weight_decay': 3.644444827141247e-06, 'step_size': 15, 'gamma': 0.8961212415687746, 'batch_size': 115}. Best is trial 0 with value: 57.0945945945946.\n",
      "[I 2024-08-14 15:26:36,082] Trial 1 finished with value: 59.797297297297305 and parameters: {'conv1_out_channels': 16, 'conv2_out_channels': 101, 'conv3_out_channels': 203, 'dropout': 0.5328428975143074, 'fc1_units': 455, 'lr': 0.00021121999261506532, 'weight_decay': 2.8547676815599713e-05, 'step_size': 6, 'gamma': 0.8594187980438937, 'batch_size': 47}. Best is trial 1 with value: 59.797297297297305.\n",
      "[I 2024-08-14 15:27:03,133] Trial 2 finished with value: 60.13513513513513 and parameters: {'conv1_out_channels': 21, 'conv2_out_channels': 61, 'conv3_out_channels': 81, 'dropout': 0.4633444097414257, 'fc1_units': 165, 'lr': 0.0007126942257014508, 'weight_decay': 1.631944558398173e-06, 'step_size': 7, 'gamma': 0.7553860490108643, 'batch_size': 117}. Best is trial 2 with value: 60.13513513513513.\n",
      "[I 2024-08-14 15:27:42,740] Trial 3 finished with value: 52.02702702702703 and parameters: {'conv1_out_channels': 47, 'conv2_out_channels': 62, 'conv3_out_channels': 211, 'dropout': 0.2840977508100292, 'fc1_units': 390, 'lr': 3.7940192562027636e-05, 'weight_decay': 1.183831361095032e-05, 'step_size': 5, 'gamma': 0.7596152490595637, 'batch_size': 118}. Best is trial 2 with value: 60.13513513513513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 60.13513513513513\n",
      "  Params: \n",
      "    conv1_out_channels: 21\n",
      "    conv2_out_channels: 61\n",
      "    conv3_out_channels: 81\n",
      "    dropout: 0.4633444097414257\n",
      "    fc1_units: 165\n",
      "    lr: 0.0007126942257014508\n",
      "    weight_decay: 1.631944558398173e-06\n",
      "    step_size: 7\n",
      "    gamma: 0.7553860490108643\n",
      "    batch_size: 117\n",
      "Epoch 1/200, Train Loss: 1.3647, Val Loss: 1.3300, Accuracy: 28.04%\n",
      "Epoch 2/200, Train Loss: 1.3296, Val Loss: 1.2970, Accuracy: 41.22%\n",
      "Epoch 3/200, Train Loss: 1.2755, Val Loss: 1.2620, Accuracy: 40.20%\n",
      "Epoch 4/200, Train Loss: 1.2768, Val Loss: 1.2354, Accuracy: 44.26%\n",
      "Epoch 5/200, Train Loss: 1.2536, Val Loss: 1.2041, Accuracy: 42.91%\n",
      "Epoch 6/200, Train Loss: 1.2241, Val Loss: 1.1873, Accuracy: 44.59%\n",
      "Epoch 7/200, Train Loss: 1.1805, Val Loss: 1.1554, Accuracy: 45.27%\n",
      "Epoch 8/200, Train Loss: 1.1464, Val Loss: 1.1216, Accuracy: 46.96%\n",
      "Epoch 9/200, Train Loss: 1.1561, Val Loss: 1.1086, Accuracy: 52.36%\n",
      "Epoch 10/200, Train Loss: 1.1302, Val Loss: 1.1006, Accuracy: 53.04%\n",
      "Epoch 11/200, Train Loss: 1.1262, Val Loss: 1.0989, Accuracy: 51.01%\n",
      "Epoch 12/200, Train Loss: 1.0918, Val Loss: 1.0798, Accuracy: 51.35%\n",
      "Epoch 13/200, Train Loss: 1.0742, Val Loss: 1.0676, Accuracy: 52.03%\n",
      "Epoch 14/200, Train Loss: 1.0502, Val Loss: 1.0544, Accuracy: 53.04%\n",
      "Epoch 15/200, Train Loss: 1.0539, Val Loss: 1.0526, Accuracy: 52.03%\n",
      "Epoch 16/200, Train Loss: 1.0122, Val Loss: 1.0552, Accuracy: 49.32%\n",
      "Epoch 17/200, Train Loss: 1.0431, Val Loss: 1.0501, Accuracy: 54.73%\n",
      "Epoch 18/200, Train Loss: 1.0161, Val Loss: 1.0259, Accuracy: 52.70%\n",
      "Epoch 19/200, Train Loss: 1.0207, Val Loss: 1.0332, Accuracy: 54.05%\n",
      "Epoch 20/200, Train Loss: 0.9978, Val Loss: 1.0357, Accuracy: 53.04%\n",
      "Epoch 21/200, Train Loss: 1.0131, Val Loss: 1.0480, Accuracy: 53.38%\n",
      "Epoch 22/200, Train Loss: 0.9671, Val Loss: 1.0174, Accuracy: 54.73%\n",
      "Epoch 23/200, Train Loss: 0.9746, Val Loss: 1.0290, Accuracy: 53.72%\n",
      "Epoch 24/200, Train Loss: 0.9794, Val Loss: 1.0257, Accuracy: 53.72%\n",
      "Epoch 25/200, Train Loss: 0.9665, Val Loss: 1.0186, Accuracy: 53.72%\n",
      "Epoch 26/200, Train Loss: 0.9708, Val Loss: 1.0024, Accuracy: 56.42%\n",
      "Epoch 27/200, Train Loss: 0.9416, Val Loss: 1.0066, Accuracy: 53.04%\n",
      "Epoch 28/200, Train Loss: 0.9369, Val Loss: 1.0145, Accuracy: 54.73%\n",
      "Epoch 29/200, Train Loss: 0.9364, Val Loss: 0.9903, Accuracy: 57.09%\n",
      "Epoch 30/200, Train Loss: 0.9376, Val Loss: 0.9897, Accuracy: 54.73%\n",
      "Epoch 31/200, Train Loss: 0.9293, Val Loss: 1.0279, Accuracy: 55.74%\n",
      "Epoch 32/200, Train Loss: 0.9426, Val Loss: 0.9864, Accuracy: 56.42%\n",
      "Epoch 33/200, Train Loss: 0.9029, Val Loss: 0.9781, Accuracy: 55.07%\n",
      "Epoch 34/200, Train Loss: 0.9434, Val Loss: 0.9754, Accuracy: 56.76%\n",
      "Epoch 35/200, Train Loss: 0.9124, Val Loss: 0.9759, Accuracy: 57.09%\n",
      "Epoch 36/200, Train Loss: 0.8983, Val Loss: 0.9676, Accuracy: 56.76%\n",
      "Epoch 37/200, Train Loss: 0.9013, Val Loss: 0.9683, Accuracy: 59.46%\n",
      "Epoch 38/200, Train Loss: 0.9250, Val Loss: 0.9817, Accuracy: 57.09%\n",
      "Epoch 39/200, Train Loss: 0.9051, Val Loss: 0.9601, Accuracy: 58.11%\n",
      "Epoch 40/200, Train Loss: 0.8983, Val Loss: 0.9549, Accuracy: 58.45%\n",
      "Epoch 41/200, Train Loss: 0.9843, Val Loss: 0.9584, Accuracy: 59.12%\n",
      "Epoch 42/200, Train Loss: 0.9270, Val Loss: 0.9669, Accuracy: 58.11%\n",
      "Epoch 43/200, Train Loss: 0.8822, Val Loss: 0.9563, Accuracy: 60.81%\n",
      "Epoch 44/200, Train Loss: 0.8695, Val Loss: 0.9566, Accuracy: 57.43%\n",
      "Epoch 45/200, Train Loss: 0.8843, Val Loss: 0.9575, Accuracy: 57.43%\n",
      "Epoch 46/200, Train Loss: 0.8988, Val Loss: 0.9652, Accuracy: 59.46%\n",
      "Epoch 47/200, Train Loss: 0.9079, Val Loss: 0.9537, Accuracy: 57.43%\n",
      "Epoch 48/200, Train Loss: 0.8758, Val Loss: 0.9535, Accuracy: 56.76%\n",
      "Epoch 49/200, Train Loss: 0.9245, Val Loss: 0.9518, Accuracy: 58.78%\n",
      "Epoch 50/200, Train Loss: 0.8692, Val Loss: 0.9617, Accuracy: 58.78%\n",
      "Epoch 51/200, Train Loss: 0.8748, Val Loss: 0.9573, Accuracy: 57.43%\n",
      "Epoch 52/200, Train Loss: 0.8506, Val Loss: 0.9621, Accuracy: 57.09%\n",
      "Epoch 53/200, Train Loss: 0.8807, Val Loss: 0.9485, Accuracy: 58.11%\n",
      "Epoch 54/200, Train Loss: 0.8422, Val Loss: 0.9537, Accuracy: 58.11%\n",
      "Epoch 55/200, Train Loss: 0.8694, Val Loss: 0.9482, Accuracy: 58.45%\n",
      "Epoch 56/200, Train Loss: 0.8754, Val Loss: 0.9477, Accuracy: 58.45%\n",
      "Epoch 57/200, Train Loss: 0.8565, Val Loss: 0.9464, Accuracy: 59.46%\n",
      "Epoch 58/200, Train Loss: 0.8378, Val Loss: 0.9408, Accuracy: 58.11%\n",
      "Epoch 59/200, Train Loss: 0.8287, Val Loss: 0.9436, Accuracy: 60.47%\n",
      "Epoch 60/200, Train Loss: 0.8499, Val Loss: 0.9426, Accuracy: 60.47%\n",
      "Epoch 61/200, Train Loss: 0.8632, Val Loss: 0.9373, Accuracy: 58.11%\n",
      "Epoch 62/200, Train Loss: 0.8437, Val Loss: 0.9355, Accuracy: 59.80%\n",
      "Epoch 63/200, Train Loss: 0.8295, Val Loss: 0.9398, Accuracy: 59.46%\n",
      "Epoch 64/200, Train Loss: 0.8699, Val Loss: 0.9466, Accuracy: 59.12%\n",
      "Epoch 65/200, Train Loss: 0.8281, Val Loss: 0.9394, Accuracy: 59.80%\n",
      "Epoch 66/200, Train Loss: 0.8642, Val Loss: 0.9394, Accuracy: 59.12%\n",
      "Epoch 67/200, Train Loss: 0.8159, Val Loss: 0.9414, Accuracy: 59.80%\n",
      "Epoch 68/200, Train Loss: 0.8353, Val Loss: 0.9422, Accuracy: 59.46%\n",
      "Epoch 69/200, Train Loss: 0.8165, Val Loss: 0.9418, Accuracy: 59.46%\n",
      "Epoch 70/200, Train Loss: 0.8323, Val Loss: 0.9406, Accuracy: 58.45%\n",
      "Epoch 71/200, Train Loss: 0.8408, Val Loss: 0.9419, Accuracy: 58.45%\n",
      "Epoch 72/200, Train Loss: 0.7903, Val Loss: 0.9457, Accuracy: 59.12%\n",
      "Epoch 73/200, Train Loss: 0.8051, Val Loss: 0.9457, Accuracy: 59.80%\n",
      "Epoch 74/200, Train Loss: 0.8250, Val Loss: 0.9421, Accuracy: 59.80%\n",
      "Epoch 75/200, Train Loss: 0.8371, Val Loss: 0.9451, Accuracy: 60.47%\n",
      "Epoch 76/200, Train Loss: 0.8279, Val Loss: 0.9433, Accuracy: 60.47%\n",
      "Epoch 77/200, Train Loss: 0.8288, Val Loss: 0.9437, Accuracy: 59.46%\n",
      "Epoch 78/200, Train Loss: 0.8305, Val Loss: 0.9405, Accuracy: 59.12%\n",
      "Epoch 79/200, Train Loss: 0.8071, Val Loss: 0.9451, Accuracy: 59.46%\n",
      "Epoch 80/200, Train Loss: 0.8084, Val Loss: 0.9414, Accuracy: 59.12%\n",
      "Epoch 81/200, Train Loss: 0.8028, Val Loss: 0.9415, Accuracy: 59.80%\n",
      "Epoch 82/200, Train Loss: 0.8214, Val Loss: 0.9460, Accuracy: 58.78%\n",
      "Early stopping at epoch 82\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import optuna\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# Constants\n",
    "IMG_SIZE = 64  # Resize all images to 64x64 pixels\n",
    "\n",
    "\n",
    "# Load images and extract Y channel\n",
    "def load_images_and_labels(image_dir, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is not None:\n",
    "                # Resize image to IMG_SIZE x IMG_SIZE\n",
    "                image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "                ycbcr_image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "                Y, _, _ = cv2.split(ycbcr_image)\n",
    "                images.append(Y)\n",
    "                labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "# Directories\n",
    "heavy_traffic_dir = 'Heavy_Traffic'\n",
    "light_traffic_dir = 'Light_Traffic'\n",
    "moderate_traffic_dir = 'Moderate_Traffic'\n",
    "no_traffic_dir = 'No_Traffic'\n",
    "\n",
    "\n",
    "# Load images and labels\n",
    "heavy_images, heavy_labels = load_images_and_labels(heavy_traffic_dir, 3)\n",
    "light_images, light_labels = load_images_and_labels(light_traffic_dir, 1)\n",
    "moderate_images, moderate_labels = load_images_and_labels(moderate_traffic_dir, 2)\n",
    "no_images, no_labels = load_images_and_labels(no_traffic_dir, 0)\n",
    "\n",
    "\n",
    "# Combine and preprocess data\n",
    "images = np.array(heavy_images + light_images + moderate_images + no_images, dtype=np.float32)\n",
    "labels = np.array(heavy_labels + light_labels + moderate_labels + no_labels)\n",
    "\n",
    "\n",
    "# Normalize the images\n",
    "images = images / 255.0\n",
    "images = images[:, np.newaxis, :, :]  # Add channel dimension\n",
    "\n",
    "\n",
    "# Convert labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Data augmentation and normalization\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image.squeeze())  # Remove channel dimension for transformation\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TrafficDataset(X_train, y_train, transform=train_transforms)\n",
    "test_dataset = TrafficDataset(X_test, y_test, transform=test_transforms)\n",
    "\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "    class CNNModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNNModel, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, trial.suggest_int('conv1_out_channels', 16, 64), kernel_size=3, padding=1)\n",
    "            self.conv2 = nn.Conv2d(trial.suggest_int('conv1_out_channels', 16, 64), trial.suggest_int('conv2_out_channels', 32, 128), kernel_size=3, padding=1)\n",
    "            self.conv3 = nn.Conv2d(trial.suggest_int('conv2_out_channels', 32, 128), trial.suggest_int('conv3_out_channels', 64, 256), kernel_size=3, padding=1)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.dropout = nn.Dropout(trial.suggest_float('dropout', 0.2, 0.7))\n",
    "            self.fc1 = nn.Linear(trial.suggest_int('conv3_out_channels', 64, 256) * (IMG_SIZE // 8) * (IMG_SIZE // 8), trial.suggest_int('fc1_units', 128, 512))\n",
    "            self.fc2 = nn.Linear(trial.suggest_int('fc1_units', 128, 512), 4)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.pool(torch.relu(self.conv1(x)))\n",
    "            x = self.pool(torch.relu(self.conv2(x)))\n",
    "            x = self.pool(torch.relu(self.conv3(x)))\n",
    "            x = x.view(-1, trial.suggest_int('conv3_out_channels', 64, 256) * (IMG_SIZE // 8) * (IMG_SIZE // 8))\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    return CNNModel()\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Create the model with trial suggestions\n",
    "    model = create_model(trial).float()\n",
    "    model.to('cuda')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=trial.suggest_loguniform('lr', 1e-5, 1e-3), weight_decay=trial.suggest_loguniform('weight_decay', 1e-6, 1e-4))\n",
    "    scheduler = StepLR(optimizer, step_size=trial.suggest_int('step_size', 5, 15), gamma=trial.suggest_float('gamma', 0.1, 0.9))\n",
    "    \n",
    "    # Training the model with early stopping\n",
    "    num_epochs = 50  # Use a lower number of epochs for faster optimization\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=trial.suggest_int('batch_size', 32, 128), shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=trial.suggest_int('batch_size', 32, 128), shuffle=False)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.cuda() \n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.cuda() \n",
    "                labels = labels.cuda()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        accuracy = correct / total * 100\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=4)\n",
    "\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print(f'  Value: {trial.value}')\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "best_model = create_model(trial).float()\n",
    "best_model.to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=trial.params['lr'], weight_decay=trial.params['weight_decay'])\n",
    "scheduler = StepLR(optimizer, step_size=trial.params['step_size'], gamma=trial.params['gamma'])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=trial.params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=trial.params['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "num_epochs = 200\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    best_model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.cuda() \n",
    "        labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    best_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.cuda() \n",
    "            labels = labels.cuda()\n",
    "            outputs = best_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    accuracy = correct / total * 100\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(best_model.state_dict(), 'model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf43315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
